{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b8e3b525-dce0-424e-b76e-e0647f449ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "import randomcolor\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import operator as ops\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "import base64\n",
    "\n",
    "rcolor = randomcolor.RandomColor()\n",
    "\n",
    "\n",
    "\n",
    "class Page:\n",
    "    def __init__(self, info, unpack=False, lead='', **kwargs):\n",
    "        if ' ' in info:\n",
    "            parts = info.split(' | ')\n",
    "        else:\n",
    "            parts = info, ''\n",
    "        self.url, self.title = parts[0], ''.join(parts[1:])\n",
    "        self.parse = urlparse(self.url)\n",
    "        self.params = parse_qs(self.parse.query)\n",
    "        self.tags = []\n",
    "        self.len = len(self.url)\n",
    "        self.archives = []\n",
    "        if unpack:\n",
    "#             if 'url' in self.params and self.params['url'][0].startswith(lead):\n",
    "            if self.url.startswith(lead):\n",
    "                nested = Page(self.params['url'][0])\n",
    "                self.url = nested.url\n",
    "                self.params |= nested.params\n",
    "        \n",
    "    def print(self):\n",
    "        print(str(self))\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ' | '.join([self.title, '; '.join(map(str, self.tags)), self.url[:100]])\n",
    "        \n",
    "class Tag:\n",
    "    def __init__(self, name=''):\n",
    "        self.name = name\n",
    "        self.color = rcolor.generate()\n",
    "        self.created = str(datetime.datetime.now())\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "        \n",
    "class Collection:\n",
    "    def __init__(self, urls=None):\n",
    "        self.urls = []\n",
    "        if urls:\n",
    "            self.add(urls)\n",
    "    \n",
    "    def load(self, path, limit=50, **kwargs):\n",
    "        data = []\n",
    "        with open(path, 'r', encoding='utf8') as file:\n",
    "            l = 0\n",
    "            for line in file:\n",
    "                data.append(line)\n",
    "                if l > limit:\n",
    "                    break\n",
    "                l += 1\n",
    "        self.add(data, **kwargs)\n",
    "    \n",
    "    def add(self, urls, **kwargs):\n",
    "        if type(urls) is str:\n",
    "            urls = [urls]\n",
    "        if type(urls) is list:\n",
    "            for url in urls:\n",
    "                if type(url) is str:\n",
    "                    self.urls.append(Page(url, **kwargs))\n",
    "                elif type(url) is Page:\n",
    "                    self.urls.append(url)\n",
    "    \n",
    "    def find(self, attr, value=None):\n",
    "        if not callable(attr):\n",
    "            attr = lambda x: getattr(x, attr) == value\n",
    "        return Collection(list(filter(attr, self.urls)))\n",
    "    \n",
    "    def tag(self, tags):\n",
    "        if type(tags) is str:\n",
    "            tags = Tag(tags)\n",
    "        if type(tags) is Tag:\n",
    "            tags = [tags]\n",
    "        for u in self.urls:\n",
    "            u.tags.extend(tags)\n",
    "        return self\n",
    "    \n",
    "    def tag_if_in(self, tags):\n",
    "        if type(tags) is str:\n",
    "            tags = [tags]\n",
    "        result = self\n",
    "        for t in tags:\n",
    "            result.find(lambda x: any(t.lower().replace(' ', '') in q for q in [x.title, x.url])).tag(t)\n",
    "        return result\n",
    "    \n",
    "    def visualize(self, property='len'):\n",
    "        summary = [getattr(u, property) for u in self.urls]\n",
    "        plt.hist(summary, bins=100)\n",
    "        \n",
    "    def download(self, limit=1, rate=1):\n",
    "        for u in self.urls[:limit]:\n",
    "            text = requests.get(u.url).text\n",
    "            u.archives.append(text)\n",
    "            time.sleep(1/rate)\n",
    "            \n",
    "    def save(self, path='./alexandria-library.txt', encoding='utf-8'):\n",
    "        text = json.dumps(self, default=vars)\n",
    "        text = zlib.compress(text.encode(encoding))\n",
    "        text = base64.b64encode(text)\n",
    "        with open(path, 'w') as f:\n",
    "#             f.write(text.decode(encoding, 'ignore'))\n",
    "            f.write(text.decode(encoding))\n",
    "#             f.write(text)\n",
    "    \n",
    "    def print(self, limit=100):\n",
    "        for u in self.urls[:limit]:\n",
    "            print(u)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.urls[i]\n",
    "\n",
    "class Rule:\n",
    "    def __init__(self, z, op, value, action):\n",
    "        if callable(z):\n",
    "            self.when = z\n",
    "        else:\n",
    "            self.when = lambda x: op(getattr(x, z), value)\n",
    "            \n",
    "    \n",
    "c = Collection()\n",
    "c.load('./may-28.txt', unpack=True, lead='chrome-extension://fiabciakcmgepblmdkmemdbbkilneeeh/park.html')\n",
    "\n",
    "# print(c.urls[100].parse)\n",
    "# c.find(lambda x: len(x.url)>1000)[5]\n",
    "# w = c.find(lambda x: 'Wikipedia' in x.title).tag('Wikipedia')\n",
    "# w[0]\n",
    "# t.created\n",
    "# c.tag(t)[0].tags[0].name\n",
    "# c[100].params\n",
    "# c.find(lambda x: x.len < 600).visualize()\n",
    "\n",
    "w = c.tag_if_in(['Wikipedia', 'Google', 'Colab', 'Stack Overflow', 'GitHub', 'Twitter', 'YouTube', 'Stack Exchange', 'Physics', 'The New York Times', 'NumPy'])\n",
    "# [([t.name for t in g.tags], g.url[-5:]) for g in w[:50]]\n",
    "\n",
    "r = Rule('url', ops.eq, 'wikipedia.org', None)\n",
    "w.download()\n",
    "# w.print()\n",
    "w.save()\n",
    "# w[1].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7360232c-68a8-4454-bf18-4d70f3e48095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w[1].params['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c232a-f596-4bc8-90c0-2b241479886e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b0d06c93-03b1-4ca3-bc64-824f9c6b4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE '"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = requests.get('https://stackoverflow.com/questions/2018026/what-are-the-differences-between-the-urllib-urllib2-urllib3-and-requests-modul')\n",
    "output.text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcbb70ac-9755-4ccb-9bcf-b52d41067ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0f2995ba-ca10-48e5-a399-75ed1c0b5f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

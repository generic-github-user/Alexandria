{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d01eb39-d060-475e-a61e-d3a3384ca82b",
   "metadata": {},
   "source": [
    "# Alexandria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565c2c8c-0a13-4464-86a5-70970942601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "import randomcolor\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import operator as ops\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "import base64\n",
    "import itertools\n",
    "import random\n",
    "from termcolor import colored\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pprint\n",
    "import webbrowser\n",
    "from pyvis.network import Network\n",
    "import uuid\n",
    "\n",
    "rcolor = randomcolor.RandomColor()\n",
    "\n",
    "def L(x, y):\n",
    "    return lambda x: y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c38c3-ca20-4108-ac86-fcae6bbe49bc",
   "metadata": {},
   "source": [
    "## Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05dd6b8d-bb60-4f71-a59e-23b22f7970fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page:\n",
    "    def __init__(self, info, collection=None, unpack=False, lead='', extract_keywords=True, graph=None, **kwargs):\n",
    "        if ' ' in info:\n",
    "            parts = info.split(' | ')\n",
    "        else:\n",
    "            parts = info, ''\n",
    "        self.url, self.title = parts[0], ''.join(parts[1:])\n",
    "        self.parse = urlparse(self.url)\n",
    "        self.params = parse_qs(self.parse.query)\n",
    "        self.tags = []\n",
    "        self.len = len(self.url)\n",
    "        self.archives = []\n",
    "        self.id = uuid.uuid4().hex\n",
    "        self.collection = collection\n",
    "        \n",
    "        if unpack:\n",
    "#             if 'url' in self.params and self.params['url'][0].startswith(lead):\n",
    "            if self.url.startswith(lead):\n",
    "                nested = Page(self.params['url'][0], collection=self.collection, graph=graph, **kwargs)\n",
    "                self.url = nested.url\n",
    "                self.parse = nested.parse\n",
    "                self.params |= nested.params\n",
    "        \n",
    "        self.keywords = None\n",
    "#         if extract_keywords:\n",
    "#             self.get_keywords()\n",
    "            \n",
    "    def get_keywords(self):\n",
    "        exclude = {'on', 'by', 'who', 'dont', 'was', 'without', 'when', 'http', 'https', 'www', 'com', 'an', 'in', 'the', 'with', 'and', 'org', 'a', 'as', 'en', 'of', 'to', 'at', 'all', 'for', 'we', 'how', 'it', 'do', 'why', 'be', 'have'}\n",
    "        terms = list(itertools.chain.from_iterable(self.replace(attr.lower(), ' ').split() for attr in [self.parse.path[1:], self.title]))\n",
    "#              and '&' not in t\n",
    "        keywords = {t for t in terms if 3 < len(t) < 50 and not t.isnumeric() and not sum(c.isdigit() for c in t) > 5 and not t.isspace()} - exclude\n",
    "        return keywords\n",
    "\n",
    "    def add_keywords(self, keywords, graph=None):\n",
    "        self.keywords = keywords\n",
    "        for k in self.keywords:\n",
    "            if k:\n",
    "                self.tag(k[0]+' [{}]'.format(k[1]), g=graph, color='orange')\n",
    "        return self\n",
    "    \n",
    "#     either pass tag objects back up through the call stack or pass collection to constructor\n",
    "    def tag(self, tags, g=None, **kwargs):\n",
    "        if type(tags) is str:\n",
    "            tags = Tag(tags, **kwargs)\n",
    "        if type(tags) is Tag:\n",
    "            tags = [tags]\n",
    "        \n",
    "#         self.tags.extend(tags)\n",
    "        for tag in tags:\n",
    "#             breakpoint()\n",
    "            self.tags.append(tag.id)\n",
    "            if self.collection:\n",
    "                self.collection.tags[tag.id] = tag\n",
    "            \n",
    "            if tag.name and self.title:\n",
    "                g.add_node(tag.name, color=tag.color)\n",
    "                g.add_edge(self.title, tag.name, weight=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def replace(self, s, n):\n",
    "        punctuation = '.,/_-:?()[]'\n",
    "        for p in punctuation:\n",
    "            s = s.replace(p, n)\n",
    "        return s\n",
    "    \n",
    "    def print(self):\n",
    "        print(str(self))\n",
    "        \n",
    "    def as_dict(self):\n",
    "        data = vars(self)\n",
    "#         TODO\n",
    "        if 'collection' in data:\n",
    "            data.pop('collection')\n",
    "        return data\n",
    "        \n",
    "    def __str__(self):\n",
    "#         '; '.join(self.keywords)\n",
    "        return ' | '.join([self.title, '; '.join(map(str, self.tags)), self.url[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b487d-a162-4972-8a15-c4466dec93cc",
   "metadata": {},
   "source": [
    "## Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ec13c19-9d18-4253-9fbd-605d8043173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag:\n",
    "    def __init__(self, name='', color=''):\n",
    "        self.name = name\n",
    "#         self.color = rcolor.generate()\n",
    "        colors = [\n",
    "            'grey',\n",
    "            'red',\n",
    "            'green',\n",
    "            'yellow',\n",
    "            'blue',\n",
    "            'magenta',\n",
    "            'cyan',\n",
    "            'white'\n",
    "        ]\n",
    "        if not color or color not in colors:\n",
    "            color = random.choice(colors)\n",
    "        self.color = color\n",
    "        self.created = str(datetime.datetime.now())\n",
    "        self.id = uuid.uuid4().hex\n",
    "        \n",
    "    def as_dict(self):\n",
    "        return vars(self)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return colored(self.name, self.color)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c23c3-084b-4f89-bb78-6beb38f8a753",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9364fe9e-a9d3-4344-b364-889ffb0b4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collection:\n",
    "    def __init__(self, urls=None, source='', encoding='utf8'):\n",
    "        self.urls = []\n",
    "        self.graph = nx.Graph()\n",
    "        self.id = uuid.uuid4().hex\n",
    "        \n",
    "        if urls:\n",
    "            self.add(urls)\n",
    "        \n",
    "        if source:\n",
    "            with open(source, 'r', encoding=encoding) as file:\n",
    "                text = file.read()\n",
    "                text = text.encode(encoding)\n",
    "                text = base64.b64decode(text)\n",
    "                text = zlib.decompress(text)\n",
    "                text = text.decode(encoding)\n",
    "#             print(text[:500])\n",
    "\n",
    "        self.tags = {}\n",
    "        self.keywords = {}\n",
    "        self.common_keywords = []\n",
    "    \n",
    "    def load(self, path, limit=50, **kwargs):\n",
    "        data = []\n",
    "        with open(path, 'r', encoding='utf8') as file:\n",
    "            l = 0\n",
    "            for line in file:\n",
    "                data.append(line)\n",
    "                if l > limit:\n",
    "                    break\n",
    "                l += 1\n",
    "        self.add(data, **kwargs)\n",
    "    \n",
    "    def add(self, urls, keywords=5, hide_labels=False, **kwargs):\n",
    "        if type(urls) is str:\n",
    "            urls = [urls]\n",
    "        if type(urls) is list:\n",
    "            for url in urls:\n",
    "                if type(url) is str:\n",
    "                    new = Page(url, collection=self, graph=self.graph, **kwargs)\n",
    "                elif type(url) is Page:\n",
    "                    new = url\n",
    "                self.urls.append(new)\n",
    "                label = ' ' if hide_labels else new.title\n",
    "                self.graph.add_node(new.title, color='green', label=label, data='Page')\n",
    "#                 print(new.url)\n",
    "                for k in new.get_keywords():\n",
    "                    if k in self.keywords:\n",
    "                        self.keywords[k][0] += 1\n",
    "                        self.keywords[k][1].append(new)\n",
    "                    else:\n",
    "                        self.keywords[k] = [1, [new]]\n",
    "                        \n",
    "        self.common_keywords = sorted(self.keywords.items(), key=lambda x: x[1][0], reverse=True)[:keywords]\n",
    "        for word in self.common_keywords:\n",
    "            for page in word[1][1]:\n",
    "                page.add_keywords([[word[0], word[1][0]]], graph=self.graph)\n",
    "                \n",
    "    \n",
    "    def find(self, attr, value=None):\n",
    "        if not callable(attr):\n",
    "            attr = lambda x: getattr(x, attr) == value\n",
    "        return Collection(list(filter(attr, self.urls)))\n",
    "    \n",
    "    def tag(self, tags):\n",
    "        if type(tags) is str:\n",
    "            tags = Tag(tags)\n",
    "        if type(tags) is Tag:\n",
    "            tags = [tags]\n",
    "        \n",
    "        for u in self.urls:\n",
    "            u.tags.extend(tags)\n",
    "            for tag in tags:\n",
    "#                 print(tag.name, u.title)\n",
    "                self.graph.add_edge(u.title, tag.name, weight=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def tag_if_in(self, tags):\n",
    "        if type(tags) is str:\n",
    "            tags = [tags]\n",
    "        result = self\n",
    "#         tags_ = [Tag(t) for t in tags]\n",
    "        tags_ = []\n",
    "        for t in tags:\n",
    "            t_ = Tag(t)\n",
    "            tags_.append(t_)\n",
    "            self.graph.add_node(t_.name, color=t_.color, data='Tag')\n",
    "        \n",
    "        for t in tags_:\n",
    "#             print(t.name)\n",
    "#             result.find(lambda x: any(t.name.lower().replace(' ', '') in q for q in [x.title, x.url])).tag(t)\n",
    "            for u in result.urls:\n",
    "                if any(t.name.lower().replace(' ', '') in q for q in [u.title, u.url]):\n",
    "                    u.tag(t, self.graph)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize(self, property='len'):\n",
    "        summary = [getattr(u, property) for u in self.urls]\n",
    "        plt.hist(summary, bins=100)\n",
    "        \n",
    "    def network(self, physics=False, display=0.5):\n",
    "        self.vis = Network(width=800, height=800, notebook=True)\n",
    "#         print(self.graph.nodes)\n",
    "        vis_graph = self.graph.copy()\n",
    "        nodes = [n for n, m in vis_graph.nodes.data('data') if m == 'Page']\n",
    "        nodes = nodes[:round(len(nodes) * (1 - display))]\n",
    "        vis_graph.remove_nodes_from(nodes)\n",
    "        self.vis.from_nx(vis_graph)\n",
    "#         self.vis.enable_physics(physics)\n",
    "        self.vis.toggle_physics(physics)\n",
    "        self.vis.repulsion(spring_length=400, spring_strength=0.01)\n",
    "#         g.barnes_hut()\n",
    "        output = self.vis.show('./library-network.html')\n",
    "#         return net\n",
    "        return output\n",
    "        \n",
    "        \n",
    "    def download(self, limit=1, rate=1):\n",
    "        for u in self.urls[:limit]:\n",
    "            print('Archiving '+u.url)\n",
    "            text = requests.get(u.url).text\n",
    "            now = time.time()\n",
    "            u.archives.append([now, text])\n",
    "            time.sleep(1/rate)\n",
    "            \n",
    "    def save(self, path='./alexandria-library.txt', encoding='utf-8', compress=True):\n",
    "        if type(compress) is bool:\n",
    "            compress = int(compress)\n",
    "        \n",
    "        attrs = ['urls', 'id', 'tags']\n",
    "        data = {}\n",
    "        for a in attrs:\n",
    "            value = getattr(self, a)\n",
    "            if type(value) is list:\n",
    "#                 value = [v.as_dict() for v in value]\n",
    "                value_ = []\n",
    "                for v in value:\n",
    "                    if type(v) in [Page, Tag]:\n",
    "                        value_.append(v.as_dict())\n",
    "                value = value_\n",
    "            elif type(value) is dict:\n",
    "                value = {k: v.as_dict() for k, v in value.items()}\n",
    "            data[a] = value\n",
    "#         text = json.dumps(self, default=vars)\n",
    "        text = json.dumps(data)\n",
    "        text = zlib.compress(text.encode(encoding), level=compress)\n",
    "        text = base64.b64encode(text)\n",
    "        with open(path, 'w') as f:\n",
    "#             f.write(text.decode(encoding, 'ignore'))\n",
    "            f.write(text.decode(encoding))\n",
    "#             f.write(text)\n",
    "        return text\n",
    "    \n",
    "    def statistics(self):\n",
    "        colors = [\n",
    "            'red',\n",
    "            'yellow',\n",
    "            'green',\n",
    "            'blue',\n",
    "            'magenta',\n",
    "            'cyan',\n",
    "            'grey',\n",
    "            'white'\n",
    "        ]\n",
    "        info = [\n",
    "            ('Number of pages', len(self.urls)),\n",
    "            ('Number of tags', len(self.common_keywords)),\n",
    "            ('Average title length', round(np.mean([len(u.title) for u in self.urls]), 1)),\n",
    "            ('Average URL length', round(np.mean([len(u.url) for u in self.urls]), 1)),\n",
    "            ('Average number of tags', round(np.mean([len(u.tags) for u in self.urls]), 1))\n",
    "        ]\n",
    "        for i, s in enumerate(info):\n",
    "            label, num = s\n",
    "            print('{}: {}'.format(colored(label, colors[i]), num))\n",
    "            \n",
    "    def random(self):\n",
    "        url = random.choice(self.urls).url\n",
    "        webbrowser.open(url)\n",
    "    \n",
    "    def print(self, limit=100):\n",
    "        print(colored('Network '+self.id, 'blue'))\n",
    "        for u in self.urls[:limit]:\n",
    "            print(u)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.urls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df33482f-181a-4335-aad8-196982102df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, z, op, value, action):\n",
    "        if callable(z):\n",
    "            self.when = z\n",
    "        else:\n",
    "            self.when = lambda x: op(getattr(x, z), value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb66d0-a952-4312-9e18-4f2d94d85ec0",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "babe8642-a51a-46be-bae0-3634026279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Collection(source='./alexandria-library.txt')\n",
    "c.load(\n",
    "    path='./may-28.txt',\n",
    "    limit=1,\n",
    "    keywords=2,\n",
    "    hide_labels=True,\n",
    "    unpack=True,\n",
    "    lead='chrome-extension://fiabciakcmgepblmdkmemdbbkilneeeh/park.html'\n",
    ")\n",
    "\n",
    "# print(c.urls[100].parse)\n",
    "# c.find(lambda x: len(x.url)>1000)[5]\n",
    "# w = c.find(lambda x: 'Wikipedia' in x.title).tag('Wikipedia')\n",
    "# w[0]\n",
    "# t.created\n",
    "# c.tag(t)[0].tags[0].name\n",
    "# c[100].params\n",
    "# c.find(lambda x: x.len < 600).visualize()\n",
    "\n",
    "# c.tag_if_in(['Wikipedia', 'Google', 'Colab', 'Stack Overflow', 'GitHub', 'Twitter', 'YouTube', 'Stack Exchange', 'Physics', 'The New York Times', 'NumPy'])\n",
    "\n",
    "# w.tag('Page')\n",
    "# [([t.name for t in g.tags], g.url[-5:]) for g in w[:50]]\n",
    "\n",
    "# r = Rule('url', ops.eq, 'wikipedia.org', None)\n",
    "# c.download(limit=3)\n",
    "# c.print()\n",
    "# c.save(compress=0)\n",
    "# w.graph.edges\n",
    "\n",
    "# c.network(physics=True, display=0.2)\n",
    "# c.statistics()\n",
    "# c.keywords.items()\n",
    "# c.random()\n",
    "\n",
    "# TODO: central tag creation buffer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "4573150b-5653-4702-85f6-011d1f1e2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(width=800, height=800, notebook=True)\n",
    "net.from_nx(w.graph)\n",
    "# net.show(\"./library-network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "a529de6b-c1c6-4ce6-80fa-2235dfdbd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pprint.PrettyPrinter()\n",
    "# p.pprint(list(c.graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b0d06c93-03b1-4ca3-bc64-824f9c6b4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = requests.get('https://stackoverflow.com/questions/2018026/what-are-the-differences-between-the-urllib-urllib2-urllib3-and-requests-modul')\n",
    "output.text[:10]\n",
    "int(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
